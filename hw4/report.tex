\documentclass[11pt]{article}

 \renewcommand*\familydefault{\sfdefault}
%%
%% to get Arial font as the sans serif font, uncomment following line:
%% \renewcommand{\sfdefault}{phv} % phv is the Arial font
\usepackage[sort,nocompress]{cite}
\usepackage[small,bf,up]{caption}
\renewcommand{\captionfont}{\footnotesize}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{graphics,epsfig,graphicx,float,subfigure,color}
%\usepackage{algorithm,algorithmic}
\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{url}
\usepackage{boxedminipage}
\usepackage[sf,bf,tiny]{titlesec}
 \usepackage[plainpages=false, colorlinks=true,
   citecolor=blue, filecolor=blue, linkcolor=blue,
   urlcolor=blue]{hyperref}

\usepackage{algorithmicx}
\usepackage{listings}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{sidecap}
\usepackage{caption}


%% Used for insertion
\usepackage[most,breakable]{tcolorbox}
\usepackage{lipsum}
\usepackage{lmodern}


\lstset{
basicstyle=\small\ttfamily,
numbers=left,
numbersep=5pt,
xleftmargin=20pt,
frame=tb,
framexleftmargin=20pt
}

\renewcommand*\thelstnumber{\arabic{lstnumber}:}

\newcommand*{\matminus}{%
  \leavevmode
  \hphantom{0}%
  \llap{%
    \settowidth{\dimen0 }{$0$}%
    \resizebox{1.1\dimen0 }{\height}{$-$}%
  }%
}

\DeclareCaptionFormat{mylst}{\hrule#1#2#3}
\captionsetup[lstlisting]{format=mylst,labelfont=bf,singlelinecheck=off,labelsep=space}

\usepackage{matlab-prettifier}
\newcommand{\todo}[1]{\textcolor{red}{#1}}
% see documentation for titlesec package
% \titleformat{\section}{\large \sffamily \bfseries}
\titlelabel{\thetitle.\,\,\,}

\renewcommand{\baselinestretch}{0.994}
\newcommand{\bs}{\boldsymbol}
\newcommand{\bfQ}{\boldsymbol{Q}}
\newcommand{\alert}[1]{\textcolor{red}{#1}}

\setlength{\emergencystretch}{20pt}

\begin{document}

\begin{center}
  \vspace*{-2cm}
  \begin{center}
  \large \textbf{%%
    Spring 2023: \\
    Advanced Topics in Numerical Analysis:\\ 
    High Performance Computing}\\
    \textbf{ Assignment 4}\\
    \textbf{Keigo Ando (ka2705)} 
  \end{center}
\end{center}  

  \hspace{3ex}


\begin{enumerate}
\item \textbf{Greene network test.} Use the pingpong example from class to test the latency and bandwidth on Greene. \\
  \hspace{2ex}

  \textbf{[solution]}  The nodes on the greene that I ran the program on are \texttt{cs482} and \texttt{cs483}. They have two Intel Xeon Platinum 8268 CPUs @ 2.90GHz each, which means that each node has 48 compute nodes. For implementation, see \texttt{pingpong.c}, which converts the original file to a c file. 
  In my experiments, each node communicated with each other in a single thread, and the latency and bandwidth were as follows
\begin{itemize}
    \item \texttt{pingpong latency: 9.907620e-04 ms}
    \item \texttt{pingpong bandwidth: 1.204634e+01 GB/s}
\end{itemize} 
You can check \texttt{pingpong.sbatch} to see how the actual job was done on greene.

\item \textbf{MPI ring communication.} Write a distributed memory program that sends an integer in a ring starting from process 0 to 1 to 2 (and so on). The last process sends the message back to process 0. Perform this loop $N$ times, where $N$ is set in the program or on the command line.$\cdots$\\
  \hspace{2ex}

  \textbf{[solution]}
  \begin{itemize}
    \item For the first part I ran the program on the greene with $N=1000$, 4 nodes and 4 processes per node. You can find my implementation in the file named \texttt{int\_ring.c}. The nodes on the greene that I ran the program on are \texttt{cs301},\texttt{cs302},\texttt{cs303}, and \texttt{cs304}. The total timing and estimated latency were as follows.
    \begin{itemize}
        \item \texttt{Result after 1000 loops: 120000}
        \item \texttt{Total time: 232.102925 s}
        \item \texttt{Estimated latency: 0.014506 s}
    \end{itemize} 
    \item For the second part, we communicate a large array of 2MByte in a ring with the same setting. You can find my implementation in the file named \texttt{int\_ring\_block.c}. The nodes on the greene that I ran the program on are the same as the previous experiment.The total timing and estimated bandwidth were as follows.
    \begin{itemize}
        \item \texttt{Total time: 229.641449 s}
        \item \texttt{Estimated bandwidth: 0.146117 GB/s}
    \end{itemize} 
    
    You can check \texttt{int\_ring.sbatch} to see how the actual jobs were done on greene for both parts.

  \end{itemize}
\item \textbf{Pick one out of the following two:}
\begin{description}
    \item(a) Implement an MPI version of the scan function we implemented in OpenMP. $\cdots$
    \item(b) Extend the MPI-parallel implementation of the 1D Jacobi smoothing from class to 2D. $\cdots$
  \end{description}
  \hspace{2ex}

  \textbf{[solution]}
  I choose (b). You can find my implementation in \texttt{jacobi.c}.
  
  \item \textbf{Pitch your final project.} Summarize your current plan for the final project.$\cdots$\\
  \hspace{2ex}

  \textbf{[Solution]}

    I will be working with Xuijin He on parallel-in-time integration. In general, most parallelization efforts for numerical solutions of partial differential equations have focused on spatial discretization, but in order to develop parallelization methods for faster computation, the parallel-in-time integration method has recently been developed as a parallelization method for temporal discretization. In our project, we deal with a typical parallel-in-time integration method called Parareal, introduced by Lions et al.\cite{LionsEtAl2001}.

Specifically, we will implement a 1D heat equation based on the Parareal method/algorithm and compare its error and timing with non-parallel implementations. We are considering using MPI and Cuda for the implementation. If progress is made, we may proceed with implementing the 2D case as well.

\end{enumerate}


\bibliographystyle{abbrv} 
\bibliography{source.bib}

\end{document}
